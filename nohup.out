+ nproc_per_node=8
+ export MASTER_ADDR=240.62.179.97
+ MASTER_ADDR=240.62.179.97
+ export MASTER_PORT=29500
+ MASTER_PORT=29500
+ shift 2
+ torchrun --standalone --nnodes=1 --nproc_per_node=8 -m verl.trainer.fsdp_sft_trainer data.train_files=/mnt/task_runtime/opc-annealing-corpus/parquet_files/traincode data.val_files=/mnt/task_runtime/opc-annealing-corpus/parquet_files/testcode data.response_key=text data.max_length=8192 data.train_batch_size=512 data.truncation=right optim.lr=5e-5 optim.lr_scheduler=wsd optim.weight_decay=0.1 optim.warmup_steps_ratio=0 '+data.response_dict_keys=[text]' data.micro_batch_size_per_gpu=32 model.partial_pretrain=ZhangShenao/Llama-3.2-1B model.use_liger=True trainer.project_name=cpt-math trainer.experiment_name=cpt-code-llama3.2-1b 'trainer.logger=[console,wandb]' trainer.default_hdfs_dir=null trainer.save_freq=2000 trainer.test_freq=-1 ulysses_sequence_parallel_size=4 use_remove_padding=true
W0630 23:37:23.620000 49263 torch/distributed/run.py:792] 
W0630 23:37:23.620000 49263 torch/distributed/run.py:792] *****************************************
W0630 23:37:23.620000 49263 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0630 23:37:23.620000 49263 torch/distributed/run.py:792] *****************************************
2025-06-30 23:37:36.029397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.043381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.047475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.058849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.273265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.280089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.287369: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.291487: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.293712: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.297874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.302727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.309109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.332448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.346134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.349567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.350289: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.361426: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.363261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.367425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.378779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.390431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.404675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.408954: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.420507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.430402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.444173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.444473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-30 23:37:36.448391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.458084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-30 23:37:36.459577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.462283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-30 23:37:36.473316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-30 23:37:36.922153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.168069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.186968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.241279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.313691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.372297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.396901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-06-30 23:37:37.425756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Normalize batch size by dp 2
Using sequence parallel size: 4
Using remove padding: True
Using SP rank 0 and size 2 for data distribution
Each SP rank gets different data, but the same data WITHIN the same rank
Using FSDP rank 0 and size 2 for data distribution
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:42,053][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:42,385][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:42,473][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:42,972][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
functools.partial(<function _or_policy at 0x7f8759e77130>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f8759e77010>, transformer_layer_cls={<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>})])
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:43,176][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:43,371][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
[2025-06-30 23:37:43,436][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
NCCL version 2.21.5+cuda12.4
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-06-30 23:37:43,677][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Total training steps: 10396
Total training steps: 10396
Total training steps: 10396
Total training steps: 10396
Total training steps: 10396
Total training steps: 10396
Total training steps: 10396
Number of steps/epoch 10396, number of epochs 1, total number of steps 10396
{'data': {'train_batch_size': 256, 'micro_batch_size': None, 'micro_batch_size_per_gpu': 32, 'train_files': '/mnt/task_runtime/opc-annealing-corpus/parquet_files/traincode', 'val_files': '/mnt/task_runtime/opc-annealing-corpus/parquet_files/testcode', 'prompt_key': 'question', 'response_key': 'text', 'prompt_dict_keys': None, 'response_dict_keys': ['text'], 'multiturn': {'enable': False, 'messages_key': 'messages', 'tools_key': 'tools', 'enable_thinking_key': 'enable_thinking'}, 'max_length': 8192, 'truncation': 'right', 'balance_dp_token': False, 'chat_template': None, 'custom_cls': {'path': None, 'name': None}, 'use_shm': False}, 'model': {'partial_pretrain': 'ZhangShenao/Llama-3.2-1B', 'use_shm': False, 'fsdp_config': {'model_dtype': 'fp32', 'wrap_policy': {'min_num_params': 0}, 'cpu_offload': False, 'offload_params': False}, 'external_lib': None, 'enable_gradient_checkpointing': True, 'trust_remote_code': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': True, 'strategy': 'fsdp2'}, 'optim': {'lr': 5e-05, 'betas': [0.9, 0.95], 'weight_decay': 0.1, 'warmup_steps_ratio': 0, 'clip_grad': 1.0, 'lr_scheduler': 'wsd'}, 'ulysses_sequence_parallel_size': 4, 'use_remove_padding': True, 'trainer': {'default_local_dir': '/mnt/task_wrapper/user_output/artifacts/checkpoints/${trainer.project_name}/${trainer.experiment_name}', 'default_hdfs_dir': None, 'resume_path': None, 'project_name': 'cpt-math', 'experiment_name': 'cpt-code-llama3.2-1b', 'total_epochs': 1, 'total_training_steps': None, 'logger': ['console', 'wandb'], 'seed': 1, 'save_freq': 2000, 'test_freq': -1, 'nnodes': 1, 'n_gpus_per_node': 8, 'max_ckpt_to_keep': None}}
wandb: Currently logged in as: shenaozhang (shenaoz) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /mnt/task_runtime/verl/wandb/run-20250630_233755-7gge3qbh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cpt-code-llama3.2-1b
wandb: ⭐️ View project at https://wandb.ai/shenaoz/cpt-math
wandb: 🚀 View run at https://wandb.ai/shenaoz/cpt-math/runs/7gge3qbh
Total training steps: 10396
Epoch 1/1:   0%|          | 0/10396 [00:00<?, ?it/s]W0630 23:44:57.397000 49263 torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGTERM death signal, shutting down workers
W0630 23:44:57.398000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49336 closing signal SIGTERM
W0630 23:44:57.398000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49337 closing signal SIGTERM
W0630 23:44:57.399000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49338 closing signal SIGTERM
W0630 23:44:57.400000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49339 closing signal SIGTERM
W0630 23:44:57.427000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49340 closing signal SIGTERM
W0630 23:44:57.459000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49341 closing signal SIGTERM
W0630 23:44:57.471000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49342 closing signal SIGTERM
W0630 23:44:57.495000 49263 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49343 closing signal SIGTERM
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 870, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 49263 got signal: 15
