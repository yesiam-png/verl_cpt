+ nproc_per_node=8
+ export MASTER_PORT=29500
+ MASTER_PORT=29500
+ shift 2
+ torchrun --standalone --nnodes=1 --nproc_per_node=8 -m verl.trainer.fsdp_sft_trainer data.train_files=/mnt/task_runtime/opc-annealing-corpus/parquet_files/traincode data.val_files=/mnt/task_runtime/opc-annealing-corpus/parquet_files/testcode data.response_key=text data.max_length=8192 data.train_batch_size=512 data.truncation=right optim.lr=5e-5 optim.lr_scheduler=wsd optim.weight_decay=0.1 optim.warmup_steps_ratio=0 '+data.response_dict_keys=[text]' data.micro_batch_size_per_gpu=32 model.partial_pretrain=ZhangShenao/Llama-3.2-1B model.use_liger=True trainer.project_name=cpt-math trainer.experiment_name=cpt-code-llama3.2-1b-realdata 'trainer.logger=[console,wandb]' trainer.default_hdfs_dir=null trainer.save_freq=2000 trainer.test_freq=-1 ulysses_sequence_parallel_size=4 use_remove_padding=true
W0708 14:39:04.540000 97248 torch/distributed/run.py:792] 
W0708 14:39:04.540000 97248 torch/distributed/run.py:792] *****************************************
W0708 14:39:04.540000 97248 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0708 14:39:04.540000 97248 torch/distributed/run.py:792] *****************************************
2025-07-08 14:39:18.809695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.824315: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.828219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.838537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.914859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.914865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.927624: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.927625: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.931501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.931501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.942414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.942413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.961521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.961519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.961538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.961562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:18.974431: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.974433: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.974424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.974457: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:18.978304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.978304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.978356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.978396: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:18.988804: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.989325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.989325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:18.989363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:19.038109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-07-08 14:39:19.051067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-07-08 14:39:19.054975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-07-08 14:39:19.065023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-08 14:39:19.787793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.804584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.804961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.846340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.866120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.887215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.924675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-07-08 14:39:19.992064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Normalize batch size by dp 2
Using sequence parallel size: 4
Using remove padding: True
Using SP rank 0 and size 2 for data distribution
Each SP rank gets different data, but the same data WITHIN the same rank
Using FSDP rank 0 and size 2 for data distribution
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
/mnt/task_runtime/verl/verl/utils/tokenizer.py:30: UserWarning: tokenizer.pad_token_id is None. Now set to 128001
  warnings.warn(f"tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}", stacklevel=1)
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:25,239][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:25,463][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
functools.partial(<function _or_policy at 0x7f0de2a6b130>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f0de2a6b010>, transformer_layer_cls={<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>})])
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:25,620][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:25,906][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:26,009][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:26,031][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
Skipping monkey patch for LlamaForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[2025-07-08 14:39:26,092][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
NCCL version 2.21.5+cuda12.4
[2025-07-08 14:39:26,152][liger_kernel.transformers.monkey_patch][INFO] - Applying Liger kernels to model instance with model type: llama with kwargs: {}
Total training steps: 0
Total training steps: 0
Total training steps: 0
Total training steps: 0
Total training steps: 0
Total training steps: 0
Total training steps: 0
Number of steps/epoch 0, number of epochs 1, total number of steps 0
{'data': {'train_batch_size': 256, 'micro_batch_size': None, 'micro_batch_size_per_gpu': 32, 'train_files': '/mnt/task_runtime/opc-annealing-corpus/parquet_files/traincode', 'val_files': '/mnt/task_runtime/opc-annealing-corpus/parquet_files/testcode', 'prompt_key': 'question', 'response_key': 'text', 'prompt_dict_keys': None, 'response_dict_keys': ['text'], 'multiturn': {'enable': False, 'messages_key': 'messages', 'tools_key': 'tools', 'enable_thinking_key': 'enable_thinking'}, 'max_length': 8192, 'truncation': 'right', 'balance_dp_token': False, 'chat_template': None, 'custom_cls': {'path': None, 'name': None}, 'use_shm': False}, 'model': {'partial_pretrain': 'ZhangShenao/Llama-3.2-1B', 'use_shm': False, 'fsdp_config': {'model_dtype': 'fp32', 'wrap_policy': {'min_num_params': 0}, 'cpu_offload': False, 'offload_params': False}, 'external_lib': None, 'enable_gradient_checkpointing': True, 'trust_remote_code': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': True, 'strategy': 'fsdp2'}, 'optim': {'lr': 5e-05, 'betas': [0.9, 0.95], 'weight_decay': 0.1, 'warmup_steps_ratio': 0, 'clip_grad': 1.0, 'lr_scheduler': 'wsd'}, 'ulysses_sequence_parallel_size': 4, 'use_remove_padding': True, 'trainer': {'default_local_dir': '/mnt/task_wrapper/user_output/artifacts/checkpoints/${trainer.project_name}/${trainer.experiment_name}', 'default_hdfs_dir': None, 'resume_path': None, 'project_name': 'cpt-math', 'experiment_name': 'cpt-code-llama3.2-1b-realdata', 'total_epochs': 1, 'total_training_steps': None, 'logger': ['console', 'wandb'], 'seed': 1, 'save_freq': 2000, 'test_freq': -1, 'nnodes': 1, 'n_gpus_per_node': 8, 'max_ckpt_to_keep': None}}
wandb: Currently logged in as: shenaozhang (shenaoz) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /mnt/task_runtime/verl/wandb/run-20250708_143940-nkoxzu72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cpt-code-llama3.2-1b-realdata
wandb: ‚≠êÔ∏è View project at https://wandb.ai/shenaoz/cpt-math
wandb: üöÄ View run at https://wandb.ai/shenaoz/cpt-math/runs/nkoxzu72
Total training steps: 0
Epoch 1/1: 0it [00:00, ?it/s]Epoch 1/1: 0it [00:00, ?it/s]
wandb:                                                                                
wandb: üöÄ View run cpt-code-llama3.2-1b-realdata at: https://wandb.ai/shenaoz/cpt-math/runs/nkoxzu72
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shenaoz/cpt-math
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250708_143940-nkoxzu72/logs
